{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "\n",
    "    def __init__(self, text, seq_len, batch_size, state_size):\n",
    "        self.text = text\n",
    "        self.text_size = len(text)\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.state_size = state_size\n",
    "        self._init_dict()\n",
    "        self.state_holders = [self._create_state_holder() for _ in range(self.batch_size)]\n",
    "\n",
    "    def _create_state_holder(self):\n",
    "        index = random.randint(0, self.text_size - self.seq_len * 10)\n",
    "        state = np.zeros(shape=(self.state_size), dtype=np.float)\n",
    "        return (index, state)\n",
    "\n",
    "    def _init_dict(self):\n",
    "        vocab = set()\n",
    "        for c in text:\n",
    "            vocab.update([c])\n",
    "        self.index2char = ['<pad>', '<go>', '<eos>'] + [c for c in vocab]\n",
    "        self.char2index = {}\n",
    "        for i, c in enumerate(self.index2char):\n",
    "            self.char2index[c] = i\n",
    "        self.vocab_size = len(self.index2char)\n",
    "\n",
    "    def _next_seq(self, ind):\n",
    "        seq = np.zeros(shape=(self.seq_len), dtype=np.int32)\n",
    "        seq_lrn = np.zeros(shape=(self.seq_len), dtype=np.int32)\n",
    "        seq[0] = self.char2index['<go>']\n",
    "        l = 0\n",
    "        for i in range(self.seq_len):\n",
    "            if ind + i >= self.text_size:\n",
    "                break\n",
    "            if i+1 < self.seq_len:\n",
    "                seq[i+1] = self.char2index[text[ind+i]]\n",
    "            seq_lrn[i] = self.char2index[text[ind+i]]\n",
    "        return (seq, seq_lrn, i+1)\n",
    "\n",
    "    def get_batch(self):\n",
    "        seqs = []\n",
    "        seq_lrns = []\n",
    "        lens = []\n",
    "        states = []\n",
    "        for i, _ in enumerate(self.state_holders):\n",
    "            if self.state_holders[i][0] >= self.text_size:\n",
    "                self.state_holders[i] = self._create_state_holder()\n",
    "            (seq, seq_lrn, seq_len) = self._next_seq(self.state_holders[i][0])\n",
    "            self.state_holders[i] = (self.state_holders[i][0] + seq_len, self.state_holders[i][1])\n",
    "            seqs.append(seq)\n",
    "            seq_lrns.append(seq_lrn)\n",
    "            lens.append(seq_len)\n",
    "            states.append(self.state_holders[i][1])\n",
    "        return (seqs, seq_lrns, lens, states)\n",
    "\n",
    "    def update_states(self, states):\n",
    "        for i, _ in enumerate(self.state_holders):\n",
    "            self.state_holders[i] = (self.state_holders[i][0], states[i])\n",
    "\n",
    "    def seq2text(self, seq):\n",
    "        return ''.join([self.index2char[i] for i in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  83\n",
      "epoches:  1000\n",
      "steps:  2860\n",
      "start epoch: 1...\n",
      "step 0, acc: (0.0, 0.0012207031)\n",
      "step 100, acc: (0.05314414, 0.052707348)\n",
      "step 200, acc: (0.03202327, 0.03191616)\n",
      "step 300, acc: (0.025483606, 0.025435433)\n",
      "step 400, acc: (0.02259134, 0.022569701)\n",
      "step 500, acc: (0.021152006, 0.021141216)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-b159e32adc88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mbatch_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 128\n",
    "state_size = 256\n",
    "epoches = 1000\n",
    "embedding_size = 128\n",
    "layers = 3\n",
    "\n",
    "with codecs.open('text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_generator = BatchGenerator(text, seq_len, batch_size, state_size)\n",
    "sample_batch_generator = BatchGenerator(text, 100, 1, state_size)\n",
    "steps_num = batch_generator.text_size // seq_len\n",
    "\n",
    "inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "lengths = tf.placeholder(tf.int32, [None], name='lengths')\n",
    "states = tf.placeholder(tf.float32, [None, state_size], name='states')\n",
    "\n",
    "embedding_table = tf.Variable(tf.random_uniform([batch_generator.vocab_size, embedding_size]))\n",
    "embedding = tf.nn.embedding_lookup(embedding_table, inputs)\n",
    "\n",
    "cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "projection_layer = tf.layers.Dense(batch_generator.vocab_size, use_bias=False)\n",
    "print('vocab size: ', batch_generator.vocab_size)\n",
    "\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(embedding, lengths)\n",
    "train_decoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, states, output_layer=projection_layer)\n",
    "train_outputs, train_states, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "train_output = train_outputs.rnn_output\n",
    "train_sample_id = train_outputs.sample_id\n",
    "\n",
    "p1 = tf.print('output: ', tf.shape(train_output))\n",
    "p2 = tf.print('sample_id: ', train_sample_id)\n",
    "p3 = tf.print('states: ', tf.shape(train_states))\n",
    "p4 = tf.print('inputs: ', inputs)\n",
    "p5 = tf.print('labels: ', tf.shape(labels))\n",
    "p6 = tf.print('lengths: ', tf.shape(lengths))\n",
    "\n",
    "masks = tf.sequence_mask(lengths=lengths, dtype=tf.float32)\n",
    "p7 = tf.print('masks: ', masks)\n",
    "#with tf.control_dependencies([p2]):\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits=train_output, targets=labels, weights=masks)\n",
    "\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss)\n",
    "accuracy = tf.metrics.accuracy(inputs, train_sample_id, masks)\n",
    "\n",
    "# prediction decoder\n",
    "#prediction_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "#    embedding=embedding,\n",
    "#    start_tokens=tf.fill([batch_size], tf.to_int32(batch_generator.char2index['<go>'])),\n",
    "#    end_token=tf.to_int32(tf.fill([], batch_generator.char2index['<eos>'])))\n",
    "\n",
    "#prediction_decoder = tf.contrib.seq2seq.BasicDecoder(cell, prediction_helper, states, output_layer=projection_layer)\n",
    "#prediction_output, _, _ = tf.contrib.seq2seq.dynamic_decode(prediction_decoder, maximum_iterations=seq_len)\n",
    "#preds = batch_generator.seq2text(tf.to_int64(prediction_output.sample_id))\n",
    "\n",
    "\n",
    "print('epoches: ', epoches)\n",
    "print('steps: ', steps_num)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(epoches):\n",
    "        print('start epoch: {}...'.format(epoch+1))\n",
    "        for step in range(steps_num):\n",
    "            (x, y, l, s) = batch_generator.get_batch()\n",
    "            _, new_states, acc = sess.run([optimize, train_states, accuracy], feed_dict={inputs: x, labels: y, lengths: l, states: s})\n",
    "            batch_generator.update_states(new_states)\n",
    "            if step % 100 == 0:\n",
    "                print('step {}, acc: {}'.format(step, acc))\n",
    "                #res = sess.run(preds, feed_dict={states: s})\n",
    "                # print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(batch, vocab_size):\n",
    "    target = []\n",
    "    input = []\n",
    "    for seq in batch:\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(seq)):\n",
    "            t_x = [0] * vocab_size\n",
    "            t_y = [0] * vocab_size\n",
    "            c_i = int(seq[i])-1\n",
    "            n_i = 0\n",
    "            if i < len(seq) - 1:\n",
    "                n_i = int(seq[i+1])-1\n",
    "            t_x[c_i] = 1    \n",
    "            t_y[n_i] = 1\n",
    "            x.append(t_x)\n",
    "            y.append(t_y)\n",
    "\n",
    "        input.append(x)\n",
    "        target.append(y)\n",
    "\n",
    "    return np.array(input), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "class SoftmaxPredictionRnn:\n",
    "\n",
    "    def __init__(self, input, target, num_hidden=64, num_layers=3):\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self._max_grad_norm = .2\n",
    "        self._learning_rate = .001\n",
    "        self._input = input\n",
    "        self._target = target\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self._input), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # Recurrent network.\n",
    "        cells = []\n",
    "        for _ in range(self._num_layers):\n",
    "            cells.append(tf.contrib.rnn.GRUCell(self._num_hidden))\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "        # Get dimensions\n",
    "        self._max_length = int(self._input.get_shape()[1])\n",
    "        self._num_classes = int(self._input.get_shape()[2])\n",
    "        batch_size = tf.shape(self._input)[0]\n",
    "\n",
    "        states = cell.zero_state(batch_size, tf.float32)\n",
    "        state_type = type(states)\n",
    "        self._initial_state = [\n",
    "            tf.placeholder_with_default(zero_state, [None, self._num_hidden]) for zero_state in states]\n",
    "        self._initial_state = state_type(self._initial_state)\n",
    "        self._zero_state = self._initial_state\n",
    "        \n",
    "        self._output, self._final_state = tf.nn.dynamic_rnn(cell, self._input,\n",
    "                                                            dtype=tf.float32, sequence_length=self.length,\n",
    "                                                            initial_state=self._initial_state)\n",
    "\n",
    "        # Softmax layer.\n",
    "        weight = tf.get_variable('W', [self._num_hidden, self._num_classes])\n",
    "        bias = tf.get_variable('b', [self._num_classes], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        # Flatten to apply same weights to all time steps.\n",
    "        output = tf.reshape(self._output, [-1, self._num_hidden])\n",
    "        self._raw_logits = tf.matmul(output, weight) + bias\n",
    "        self._logits = tf.nn.softmax(tf.matmul(output, weight) + bias)\n",
    "        prediction = tf.reshape(self._logits, [-1, self._max_length, self._num_classes])\n",
    "\n",
    "        tf.summary.histogram(\"rnn_output\", output)\n",
    "        for w in cell.weights:\n",
    "            tf.summary.histogram(\"rnn_weight\", w)\n",
    "        tf.summary.histogram(\"softmax_w\", weight)\n",
    "        tf.summary.histogram(\"softmax_bias\", bias)\n",
    "        tf.summary.histogram(\"prediction\", prediction)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        # Compute cross entropy for each frame.\n",
    "        cross_entropy = self._target * tf.log(self.prediction)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self._target), reduction_indices=2))\n",
    "        cross_entropy *= mask\n",
    "\n",
    "        # Average over actual sequence lengths.\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "        cross_entropy /= tf.cast(self.length, tf.float32)\n",
    "\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads = tf.gradients(self.cost, tvars)\n",
    "        clip_grads, _ = tf.clip_by_global_norm(grads, self._max_grad_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "\n",
    "        #tf.summary.histogram(\"gradients\", grads)\n",
    "        #tf.summary.histogram(\"clip_gradients\", clip_grads)\n",
    "\n",
    "        return optimizer.apply_gradients(zip(clip_grads, tvars))\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(tf.argmax(self._target, 2), tf.argmax(self.prediction, 2))\n",
    "        mistakes = tf.cast(mistakes, tf.float32)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self._target), reduction_indices=2))\n",
    "        mistakes *= mask\n",
    "\n",
    "        # Average over actual sequence lengths.\n",
    "        mistakes = tf.reduce_sum(mistakes, reduction_indices=1)\n",
    "        mistakes /= tf.cast(self.length, tf.float32)\n",
    "        mistake = tf.reduce_mean(mistakes)\n",
    "\n",
    "        tf.summary.scalar('error', mistake)\n",
    "\n",
    "        return mistake\n",
    "\n",
    "\n",
    "    def train_epoch(self, session, batch_generator, model, epoch, steps, x, y):\n",
    "        batch = batch_generator.start()\n",
    "        batch_x, batch_y = gen_batch(batch, batch_generator._vocab_size)\n",
    "        state = session.run(model._zero_state, feed_dict={self._input: batch_x})\n",
    "        \n",
    "        for step in range(steps):\n",
    "            _, state, s = sess.run([model.optimize, model._final_state, summaries],\n",
    "                                   feed_dict={x: batch_x, y: batch_y, model._initial_state: state})\n",
    "            writer.add_summary(s, epoch * steps + step)\n",
    "            batch = batch_generator.get_batch()\n",
    "            batch_x, batch_y = gen_batch(batch, batch_generator._vocab_size)\n",
    "\n",
    "    def sample(self, session, start_text, length, temperature=1., max_prob=True):\n",
    "        def get_input(symbol, seq_len, vocab_size):\n",
    "            input = []\n",
    "            one_hot_one = [0.] * vocab_size\n",
    "            one_hot_one[int(symbol)-1] = 1.\n",
    "            \n",
    "            seq = []\n",
    "            seq.append(one_hot_one)\n",
    "            for _ in range(1, seq_len):\n",
    "                seq.append([0.] * vocab_size)\n",
    "\n",
    "            input.append(seq)\n",
    "            return np.array(input, dtype=np.float32)\n",
    "\n",
    "        # Prepare network's state to generate\n",
    "        x = get_input(start_text[0], self._max_length, self._num_classes)\n",
    "        state = session.run(self._zero_state, feed_dict={self._input: x})\n",
    "        sample = start_text[0]\n",
    "        for char in start_text[:-1]:\n",
    "            x = get_input(char, self._max_length, self._num_classes)\n",
    "            state = session.run(self._final_state, {self._input: x, self._initial_state: state})\n",
    "            \n",
    "        # Generate symbols\n",
    "        x = get_input(start_text[-1], self._max_length, self._num_classes)\n",
    "        seq = []\n",
    "        \n",
    "        for i in range(length):\n",
    "            state, logits = session.run([self._final_state, self._logits],\n",
    "                                        {self._input: x, self._initial_state: state})\n",
    "\n",
    "            sample = np.argmax(logits[0]) + 1\n",
    "            seq.append(sample)\n",
    "            x = get_input(sample, self._max_length, self._num_classes)\n",
    "\n",
    "        return seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
