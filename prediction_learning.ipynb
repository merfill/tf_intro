{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Softmax\n",
    "\n",
    "## Regression versus Classification\n",
    "\n",
    "In statistics we have two different names for tasks that map some input features to some output value:\n",
    "* We use word **regression** when the output is real-valued.\n",
    "* We use word **classification** when the output is one of discrete set of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "The target function is sume of feature values multiplied to some weights:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=0}^N w_i \\times f_i\n",
    "$$\n",
    "\n",
    "We have $w_i$ as weight and feature value $f_i$ for feature $i$. Weight $w_0$ is free member with $f_0 = 1$.\n",
    "We can also reformulate the expression above by using notion of dot product. For two vectors $a = (a_1, \\ldots, a_n)$ and $b = (b_1, \\ldots, b_n)$ **dot product** is defined as following:\n",
    "\n",
    "$$\n",
    "a\\cdot b = \\sum_{i=1}^n a_i b_i\n",
    "$$\n",
    "\n",
    "So, linear regression can be reformilated as:\n",
    "\n",
    "$$\n",
    "y = w \\cdot f\n",
    "$$\n",
    "\n",
    "where $w$ and $f$ are vectors: $w = (w_0, w_1, \\ldots, w_N)$ and $f = (1, f_1, \\ldots, f_N)$, respectively.\n",
    "\n",
    "To learn linear regression one can use gradient descent optimization or something more specific. Loss function is obviously squared error:\n",
    "\n",
    "$$\n",
    "cost(w) = \\sum_{j=0}^M (y_{pred}^{(j)} - y_{obs}^{(j)})^2\n",
    "$$\n",
    "\n",
    "Here $M$ is the size of the train data, $y_{obs}^{(j)}$ is observed value for $j$-th example in the train collection and $y_{pred}^{(j)}$ is a value, calculated by using linear regression formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression applies linear regression to classification task. The problem here is that output values are discrete and we cannot calculate derivatives on this.\n",
    "\n",
    "Consider the simplest case of binary classification: classifying whether some observation is in the class (true) or not in the class (false). Instead of returning 0 and 1 we'd like to have **probability** that a particular observation is in class 0 or 1.\n",
    "\n",
    "If we'll modify linear regression formula for this case we'll have the following:\n",
    "\n",
    "$$\n",
    "P(y=true|x) = \\sum_{i=0}^N w_i \\times f_i = w \\cdot f\n",
    "$$\n",
    "\n",
    "This formula is not good because of the expression $w \\times f$ can have potentionally infinite values, not probabilities. So, we'll instead learn **ratio** of two probabilities, which is called **odds**:\n",
    "\n",
    "$$\n",
    "\\frac{p(y=true|x)}{1-p(y=true|x)} = w \\cdot f\n",
    "$$\n",
    "\n",
    "But, the value of this expression can be only nonnegative, we need to cover all real values. So, instead learn logarithm:\n",
    "\n",
    "$$\n",
    "ln(\\frac{p(y=true|x)}{1-p(y=true|x)}) = w \\cdot f\n",
    "$$\n",
    "\n",
    "Left part of the expression is called **logit** functiong:\n",
    "\n",
    "$$\n",
    "logit(p(x)) = ln(\\frac{p(y=true|x)}{1-p(y=true|x)}) \n",
    "$$\n",
    "\n",
    "The model of regression in which we use a linear function to estimate the logit of the probability rather then probability is knows as **logistic regression**.\n",
    "\n",
    "Calculate probability of the class from the formula of logistic regression:\n",
    "\n",
    "$$\n",
    "ln({\\frac{p(y=true|x)}{1-p(y=true|x)}}) = w \\cdot f\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{p(y=true|x)}{1-p(y=true|x)} = e^{w \\cdot f}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y=true|x) = \\frac{e^{w \\cdot f}}{1+e^{w \\cdot f}} = \\frac{1}{1+e^{-{w \\cdot f}}}\n",
    "$$\n",
    "\n",
    "It's easy to derive probability of observation to not be in the class:\n",
    "\n",
    "$$\n",
    "p(y=false|x) = \\frac{1}{1+e^{w \\cdot f}}\n",
    "$$\n",
    "\n",
    "The graph of the logit function ($e^{w \\cdot f} = z$):\n",
    "\n",
    "<img src=\"logit.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Learning in Logistic Regression\n",
    "\n",
    "Obviously, people use **conditional maximum likelihood estimation** instead of sum-squared loss function in linear regression. That means one chooses the parameters $w$ that make the probability of observed $y$ values in the training data to be highest, given the observations $x$. \n",
    "\n",
    "$$\n",
    "w' = argmax_w P(y^{(i)}|x^{(i)})\n",
    "$$\n",
    "\n",
    "For all train dataset:\n",
    "\n",
    "$$\n",
    "w' = argmax_w \\prod_i P(y^{(i)}|x^{(i)})\n",
    "$$\n",
    "\n",
    "We move to logarithm:\n",
    "\n",
    "$$\n",
    "w' = argmax_w \\sum_i log P(y^{(i)}|x^{(i)})\n",
    "$$\n",
    "\n",
    "More explicitly:\n",
    "\n",
    "$$\n",
    "w' = argmax_w \\sum_i y^{(i)}\\, log\\, P(y^{(i)} = 1|x^{(i)}) + (1 - y^{(i)})\\, log\\, P(y^{(i)} = 0|x^{(i)})\n",
    "$$\n",
    "\n",
    "Last expression is obviously called as **cross entropy** and used in neural networks to learn *logit* + *softmax* layers. Rewrite this expression in terms of logit function:\n",
    "\n",
    "$$\n",
    "w' = argmax_w \\sum_i y^{(i)}\\, log\\, \\frac{1}{1+e^{-{w \\cdot f}}} + (1 - y^{(i)})\\, log\\, \\frac{1}{1+e^{w \\cdot f}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Softmax function is generalization of logit function for several classes. For probability of $y$ be in class $c \\in C = \\{ c_1, c_2, \\ldots, c_n \\}$ We have base expression:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{Z} e^{w\\cdot f}\n",
    "$$\n",
    "\n",
    "Define normalization value $Z$ as sum of all exponents for all classes in $C$:\n",
    "\n",
    "$$\n",
    "Z = \\sum_C p(c|x) = \\sum_{c' \\in C} e^{w_{c'} \\cdot f} = \\sum_{c' \\in C} exp \\sum_{i=0}^N w_{c'_i}f_i\n",
    "$$\n",
    "\n",
    "So, to calculate softmax one firstly needs to calculate exponents, sume them and then divide each exoponent on this sum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax in Neural Networks\n",
    "\n",
    "In neural networks we have a bit different terminology:\n",
    "* Logit layer is the layer under softmax one. Outputs of this layer are used to calculate softmax and obviously are sums without activation function. Number of units are equal to number of classes.\n",
    "* Softmax layer is used to calculate softmax over logit layer outputs.\n",
    "\n",
    "Both layers together are also called **projection** layer.\n",
    "\n",
    "<img src=\"softmax.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "### Word Meaning\n",
    "\n",
    "Here we beleave that meaning of a word in the text is generally defined by its environment in this text. So, one can inherit meaning a word sense by collecting all context around a word in texts.\n",
    "\n",
    "In word2vec we represent meaning of the word by using vector of real numbers: **embedding vector**. Usually, it's enough to have 128 numbers in the vector to have good meaning representation.\n",
    "\n",
    "### Learning Word Contexts\n",
    "\n",
    "Word2vec uses two models to collect word contexts (meanings):\n",
    "1. Continues Bag of words (CBOW) model, where we learn to predict word from its environment.\n",
    "2. Skip-gram model, where we learn to predict environment from the word.\n",
    "\n",
    "Let's text is (each letter represents word):\n",
    "\n",
    "**abcddaabcadbe**\n",
    "\n",
    "Examples words and their contexts:\n",
    "\n",
    "*c* **abdd**\n",
    "\n",
    "*d* **bcda**\n",
    "\n",
    "*d* **cdaa**\n",
    "\n",
    "CBOW model: predict *c* from **abdd**\n",
    "\n",
    "Skip-Gram model: predict **abdd** from *c*\n",
    "\n",
    "<img src=\"word2vec_models.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Neural Network for Skip-Gram\n",
    "\n",
    "We use one-hot encoding for words. One hidden layer with only sums, not activation function. We use projection layer with words as classes.\n",
    "\n",
    "<img src=\"skip_gram_net_arch.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Where one can find word meaning here? Remember, word2vec uses vectors of numbers as representation of word meanings. So, these numbers are weights of connections from hidden layer to units of words. Number of such weights are equal to number of neurons in the hidden layer.\n",
    "\n",
    "<img src=\"word2vec_weight_matrix_lookup_table.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Hidden layer summators do not realy sum input weights because of one-hot enconding. So, the operation of calculation on the first layer is very efficient.\n",
    "\n",
    "<img src=\"matrix_mult_w_one_hot.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Then we can calculate dot product between selected input weights and weights of connections to a word:\n",
    "\n",
    "<img src=\"output_weights_function.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Softmax Layer Calculation\n",
    "\n",
    "If size of the vocabulary of words is big enough it's very inefficient to calculate softmax layer output. We need to calculate $e^{w_{c'} \\cdot f}$ for each word $c'$ firstly although we need to take in account only words in the context of the input word. There are two popular methods to make such calculations easy:\n",
    "* Hierarchical softmax, where we use hierarchical (binary tree based) structure to calculate softmax probabilities of words and we need only $O(log\\,|V|)$ time for this process.\n",
    "* Negative sampling by using contexts that are not exising in the text as negative examples and learning logistic regression for this network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
