{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Softmax\n",
    "\n",
    "## Regression versus Classification\n",
    "\n",
    "In statistics we have two different names for tasks that map some input features to some output value:\n",
    "* We use word **regression** when the output is real-valued.\n",
    "* We use word **classification** when the output is one of discrete set of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "The target function is sume of feature values multiplied to some weights:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=0}^N w_i \\times f_i\n",
    "$$\n",
    "\n",
    "We have $w_i$ as weight and feature value $f_i$ for feature $i$. Weight $w_0$ is free member with $f_0 = 1$.\n",
    "We can also reformulate the expression above by using notion of dot product. For two vectors $a = (a_1, \\ldots, a_n)$ and $b = (b_1, \\ldots, b_n)$ **dot product** is defined as following:\n",
    "\n",
    "$$\n",
    "a\\cdot b = \\sum_{i=1}^n a_i b_i\n",
    "$$\n",
    "\n",
    "So, linear regression can be reformilated as:\n",
    "\n",
    "$$\n",
    "y = w \\cdot f\n",
    "$$\n",
    "\n",
    "where $w$ and $f$ are vectors: $w = (w_0, w_1, \\ldots, w_N)$ and $f = (1, f_1, \\ldots, f_N)$, respectively.\n",
    "\n",
    "To learn linear regression one can use gradient descent optimization or something more specific. Loss function is obviously squared error:\n",
    "\n",
    "$$\n",
    "cost(w) = \\sum_{j=0}^M (y_{pred}^{(j)} - y_{obs}^{(j)})^2\n",
    "$$\n",
    "\n",
    "Here $M$ is the size of the train data, $y_{obs}^{(j)}$ is observed value for $j$-th example in the train collection and $y_{pred}^{(j)}$ is a value, calculated by using linear regression formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression applies linear regression to classification task. The problem here is that output values are discrete and we cannot calculate derivatives on this.\n",
    "\n",
    "Consider the simplest case of binary classification: classifying whether some observation is in the class (true) or not in the class (false). Instead of returning 0 and 1 we'd like to have **probability** that a particular observation is in class 0 or 1.\n",
    "\n",
    "If we'll modify linear regression formula for this case we'll have the following:\n",
    "\n",
    "$$\n",
    "P(y=true|x) = \\sum_{i=0}^N w_i \\times f_i = w \\cdot f\n",
    "$$\n",
    "\n",
    "This formula is not good because of the expression $w \\times f$ can have potentionally infinite values, not probabilities. So, we'll instead learn **ratio** of two probabilities, which is called **odds**:\n",
    "\n",
    "$$\n",
    "\\frac{p(y=true|x)}{1-p(y=true|x)} = w \\cdot f\n",
    "$$\n",
    "\n",
    "But, the value of this expression can be only nonnegative, we need to cover all real values. So, instead learn logarithm:\n",
    "\n",
    "$$\n",
    "ln(\\frac{p(y=true|x)}{1-p(y=true|x)}) = w \\cdot f\n",
    "$$\n",
    "\n",
    "Left part of the expression is called **logit** functiong:\n",
    "\n",
    "$$\n",
    "logit(p(x)) = ln(\\frac{p(y=true|x)}{1-p(y=true|x)}) \n",
    "$$\n",
    "\n",
    "The model of regression in which we use a linear function to estimate the logit of the probability rather then probability is knows as **logistic regression**.\n",
    "\n",
    "Calculate probability of the class from the formula of logistic regression:\n",
    "\n",
    "$$\n",
    "ln({\\frac{p(y=true|x)}{1-p(y=true|x)}}) = w \\cdot f\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{p(y=true|x)}{1-p(y=true|x)} = e^{w \\cdot f}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y=true|x) = \\frac{e^{w \\cdot f}}{1+e^{w \\cdot f}} = \\frac{1}{1+e^{-{w \\cdot f}}}\n",
    "$$\n",
    "\n",
    "It's easy to derive probability of observation to not be in the class:\n",
    "\n",
    "$$\n",
    "p(y=false|x) = \\frac{1}{1+e^{w \\cdot f}}\n",
    "$$\n",
    "\n",
    "The graph of the logit function ($e^{w \\cdot f} = z$):\n",
    "\n",
    "<img src=\"logit.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Softmax function is generalization of logit function for several classes. For probability of $y$ be in class $c \\in C = \\{ c_1, c_2, \\ldots, c_n \\}$ We have base expression:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{Z} e^{w\\cdot f}\n",
    "$$\n",
    "\n",
    "Define normalization value $Z$ as sum of all exponents for all classes in $C$:\n",
    "\n",
    "$$\n",
    "Z = \\sum_C p(c|x) = \\sum_{c' \\in C} e^{w_{c'} \\cdot f} = \\sum_{c' \\in C} exp \\sum_{i=0}^N w_{c'_i}f_i\n",
    "$$\n",
    "\n",
    "So, to calculate softmax one firstly needs to calculate exponents, sume them and then divide each exoponent on this sum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax in Neural Networks\n",
    "\n",
    "In neural networks we have a bit different terminology:\n",
    "* Logit layer is the layer under softmax one. Outputs of this layer are used to calculate softmax.\n",
    "* Softmax layer (also called projection layer) is used to calculate softmax over logit layer outputs.\n",
    "\n",
    "<img src=\"softmax.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
