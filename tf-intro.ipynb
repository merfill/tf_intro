{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations on Computation Graph\n",
    "\n",
    "## Computation Graphs\n",
    "\n",
    "Let's consider expression:\n",
    "$$e=(a+b)∗(b+1)$$\n",
    "Introduce new variables to build computation graph basing on this expression:\n",
    "$$c=a+b$$\n",
    "$$d=b+1$$\n",
    "$$e=c∗d$$\n",
    "\n",
    "Build computation graph for this expression:\n",
    "\n",
    "<img src=\"comp-graph.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "To run forward propagetion on a computation graph, set all variables in leaf nodes to some values. Then calculate values of other node basing on computational relations between child and parent nodes.\n",
    "\n",
    "<img src=\"comp-graph-eval.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Back Propagation\n",
    "\n",
    "Firstly, calculate derivations of nodes and extend computation graph with them by adding back (derivative) relations to nodes. Then one can calculate node values by going by direction from the top node of the graph to its leaf nodes.\n",
    "\n",
    "<img src=\"comp-graph-derivs.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "PLease, take a look on variable $b$ node. To compute derivation one needs to get values of two nodes $c$ and $d$ and sum them. For more complex expressions with more variables such sum may be very sofisticated. To not compute values of $c$ and $d$ again and again one need to **cache** them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Computation Model\n",
    "\n",
    "TensorFlow builds computation graph by basing on your Python code, where graph nodes are defined. Leaf nodes are variables and constants, internal nodes are operations. An each node in the computation graph has its own unique identifier and its value is computed only once and cached after the computation.\n",
    "\n",
    "TensorFlow extend an each node in the computation graph by adding back derivative relation to child nodes. An each node in the graph has relations to child nodes to compute in forward propagation and relations to its parent nodes to compute back propagation.\n",
    "\n",
    "## Using of TensorFlow\n",
    "\n",
    "#### The first phase:\n",
    "Define computation graph in your code by using operations and variables from tf namespace.\n",
    "\n",
    "#### The second phase:\n",
    "Create ccalculation session to TF server and run computations on the graph by passing values of leaf nodes and names of top nodes of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(2.)\n",
    "b = tf.constant(3.)\n",
    "r = tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "e =  6.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.Variable(2., name='a', dtype=tf.float32)\n",
    "b = tf.Variable(1., name='b', dtype=tf.float32)\n",
    "c = tf.add(a, b)\n",
    "d = tf.add(b, tf.constant(1.))\n",
    "e = tf.multiply(c, d)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print('e = ', sess.run(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network\n",
    "\n",
    "\n",
    "## Formula for one neuron\n",
    "\n",
    "<img src=\"artificial_neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "For each neuron in the layer we have:\n",
    "\n",
    "Input: $\\boldsymbol{x} = \\{ x_1, x_2, \\ldots, x_n \\}$\n",
    "\n",
    "Weights: $\\boldsymbol{w} = \\{ w_1, w_2, \\ldots, w_n \\}$\n",
    "\n",
    "Bias: $b$\n",
    "\n",
    "Sum: $s = \\sum_{i=1}^n w_ix_i + b = \\boldsymbol{w} \\boldsymbol{x} + b$\n",
    "\n",
    "Activation function: $y = f(s)$\n",
    "\n",
    "## Network Layer\n",
    "\n",
    "<img src=\"ffn.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "The NN layer can be desribed by:\n",
    "\n",
    "Matrix $W$ with rows as vectors of $\\boldsymbol{w}$ weights for each neuron in the layer\n",
    "\n",
    "Vector $\\boldsymbol{b}$ of bias for each neuron in the layer\n",
    "\n",
    "Sum is implemented as multiplication of matrix $W^T$ to input vector $\\boldsymbol{x}$\n",
    "\n",
    "Vector $\\boldsymbol{b}$ represents activation functions for all neurons in the layer\n",
    "\n",
    "\n",
    "## TensorFlow representation for n-dim values\n",
    "\n",
    "One may notice that in NN we have different dimension values:\n",
    "* Vectors of input, bias and activation functions.\n",
    "* Matrix of weights.\n",
    "* For more complex configurations of NN one has to operate to 3 or more dimensional values.\n",
    "\n",
    "So, TF introduced notion of **tensor** to operate values of different dimention. Tensors like numpy arrays, but they are used inside TF server during execution circle in forward and back propagation calculations. It's possible to pass numpy array as the input values of computation graph before running calculation of the graph inside a session.\n",
    "\n",
    "Tensors have shape, which defines dimension of the tensor. For example, any number has shape=() - 0-dimesional. Vectors have shape=(N), where N is length of the vector. Matrices have shape=(N,M), where N and M are matrix dimensions.\n",
    "\n",
    "One needs a way to pass values as numpy arrays (or something else) to the graph to run computations on the same graph but different values. TensoFlow provides following ways to do it:\n",
    "1. Use predefined values in constants and variable as it was demostrated before.\n",
    "2. Use placeholders.\n",
    "3. Use special data structures to fast value feeding (tf.data.Dataset).\n",
    "\n",
    "### Using Placeholders\n",
    "\n",
    "One should declared placeholder exactly like a variable, by defining shape of the data to keep in the placeholder. Then one can use this placeholder anywhere in the operations of program code. The placeholder node in the computation graph doesn't contain any value yet. One shoul pass (**feed**) this value to placeholder when the computation session is running. TF Session run method has additional parameter 'feed_dict', which is Python dict where one ca pass values by assigning them to placeholders.\n",
    "\n",
    "Let's take a look on the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e =  6.0\n",
      "e =  77.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.placeholder(dtype=tf.float32, shape=())\n",
    "b = tf.placeholder(dtype=tf.float32, shape=())\n",
    "c = tf.add(a, b)\n",
    "d = tf.add(b, tf.constant(1.))\n",
    "e = tf.multiply(c, d)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print('e = ', sess.run(e, feed_dict={a: 2., b: 1.}))\n",
    "    print('e = ', sess.run(e, feed_dict={a: 5., b: 6.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Learning\n",
    "## Gradient Descent\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "> Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead\n",
    "\n",
    "> Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task\n",
    "\n",
    "> The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning\n",
    "\n",
    "Neural Networks (NN) also are ML models, where optimization parameters are weights and biases for each NN layers. The most popular learning algorithm for NN is Gradient Descent, which tries to minimize error by substructing gradients on the network layers' weights and biases.\n",
    "\n",
    "<img src=\"gd.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "TensorFlow provides a variety of optimization algorithm implementations. The all are inherited from tf.train.Optimizer class. The class provides two method for optimization:\n",
    "* minimize method is the usual way to optimize NN parameters.\n",
    "* compute_gradients and apply_gradients are two part of minimize methods. One can, for instance, clip calculated gradients before applying them to parameters to escape gradient explosion (obviously, for RNN networs).\n",
    "\n",
    "List some of optimizer implementations:\n",
    "* tf.train.GradientDescentOptimizer - classic optimization algorithm. The algorithm can be successfully applied to many tasks, but it's recomended to use more sofisticated methods.\n",
    "* tf.train.MomentumOptimizer implements momentum algorithmm to work better on plateus.\n",
    "* tf.train.AdamOptimizer is the implementation of Adam algorithm, which is suitable for at least all cases. It's recomended to use.\n",
    "\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "One needs to know error function's formula to understand how to calculate gradients on NN parameters. This formula connects prediction errors calculated on a train collection and the network. So, NN with error function is called extended neural network. Error function is also called as **loss** function.\n",
    "\n",
    "<img src=\"extended_fnn.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "There are two most popular loss functions:\n",
    "* Square (quadratic) error lost function. The formula is: $\\lambda(\\boldsymbol{x})=\\frac{1}{2}\\sum_i (t_i-x_i)^2$. It's obviously used for continuous values. One of the implementatoin in TF is named as tf.losses.mean_squared_error.\n",
    "* Cross entropy is used to learn on categorial values (finite set of labels). The formula is: $H(\\boldsymbol{x})=-\\sum_i x_i * log(t_i)$. Usually, one uses cross entropy with softmax: cross entropy is used for learning puprposes, but softmax is used for prediction.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Suppose, one has train data collection $\\boldsymbol{x} = \\{ x_1, x_2, \\ldots, x_n\\}$. The question is: how to calculate the error and apply gradients? Ther are following options:\n",
    "1. Calculate the error on one train example and then update NN parameters basing on this error. It's called Stochastic Gradient Descent.\n",
    "2. Calculate error on all train collection and then apply gradients.\n",
    "3. Randomly select K examples from train collection, do forward propagation and calculate error on them, then apply gradients. This technics is known as **mini-batches** and it's used in TF at least at all cases.\n",
    "\n",
    "<img src=\"sgd.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Prediction\n",
    "\n",
    "## MNIST Database\n",
    "\n",
    "<img src=\"mnist-examples.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Train data collection contains 60000 examples, test collection - 10000. Each picture is $28\\times 28$ gray scaled image. \n",
    "\n",
    "\n",
    "## Feed Forward Network for MNIST Prediction\n",
    "\n",
    "<img src=\"ffn-mnist.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "* $X$ - flattened vector of $28\\times 28 = 784$ length.\n",
    "* $Y$ - labels as hot-vector. For instance, 3 = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
    "* $W_1$, $b_1$ - weights and bias for the first layer.\n",
    "* $W_1$, $b_1$ - weights and bias for the second layer.\n",
    "* $H = ReLU(X\\times W_1 + b_1)$\n",
    "* $O = Softmax(H\\times W_2 + b_2)$\n",
    "\n",
    "<img src=\"relu.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"softmax.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-3985ef0680a9>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Step 0 Current Training Accuracy: 0.080\n",
      "Step 100 Current Training Accuracy: 0.830\n",
      "Step 200 Current Training Accuracy: 0.900\n",
      "Step 300 Current Training Accuracy: 0.900\n",
      "Step 400 Current Training Accuracy: 0.940\n",
      "Step 500 Current Training Accuracy: 0.940\n",
      "Step 600 Current Training Accuracy: 0.930\n",
      "Step 700 Current Training Accuracy: 0.950\n",
      "Step 800 Current Training Accuracy: 1.000\n",
      "Step 900 Current Training Accuracy: 0.950\n",
      "Test Accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE = 784, 100, 10 \n",
    "\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "# Input placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, INPUT_SIZE))  \n",
    "Y = tf.placeholder(tf.float32, shape=(None, OUTPUT_SIZE))\n",
    "\n",
    "# Hidden layer weights and bias\n",
    "W_1 = tf.get_variable(\"Hidden_W\", shape=[INPUT_SIZE, HIDDEN_SIZE], initializer=initializer)\n",
    "b_1 = tf.get_variable(\"Hidden_b\", shape=[HIDDEN_SIZE], initializer=initializer)\n",
    "\n",
    "# Hidden layes sum and activation\n",
    "hidden = tf.nn.relu(tf.matmul(X, W_1) + b_1)\n",
    "\n",
    "# Output layer weights and bias\n",
    "W_2 = tf.get_variable(\"Output_W\", shape=[100, 10], initializer=initializer)\n",
    "b_2 = tf.get_variable(\"Output_b\", shape=[10], initializer=initializer)\n",
    "\n",
    "# Output layer sum\n",
    "output = tf.matmul(hidden, W_2) + b_2\n",
    "\n",
    "# Loss as crossentropy with softmax\n",
    "loss = tf.losses.softmax_cross_entropy(Y, output)\n",
    "\n",
    "# Accuracy for prediction\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Optimizer is Adam with default learing rate\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Run session\n",
    "BATCH_SIZE, NUM_TRAINING_STEPS = 100, 1000\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables in the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(NUM_TRAINING_STEPS):\n",
    "        batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "        curr_acc, _ = sess.run([accuracy, train_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "        if i % 100 == 0:\n",
    "            print('Step {} Current Training Accuracy: {:.3f}'.format(i, curr_acc))\n",
    "    \n",
    "    # Evaluate on Test Data\n",
    "    print('Test Accuracy: {:.3f}'.format(sess.run(accuracy, feed_dict={X: mnist.test.images, \n",
    "                                                                Y: mnist.test.labels})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
