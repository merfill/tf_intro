{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations on Computation Graph\n",
    "\n",
    "## Computation Graphs\n",
    "\n",
    "Let's consider expression:\n",
    "$$e=(a+b)∗(b+1)$$\n",
    "Introduce new variables to build computation graph basing on this expression:\n",
    "$$c=a+b$$\n",
    "$$d=b+1$$\n",
    "$$e=c∗d$$\n",
    "\n",
    "Build computation graph for this expression:\n",
    "\n",
    "<img src=\"comp-graph.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "To run forward propagetion on a computation graph, set all variables in leaf nodes to some values. Then calculate values of other node basing on computational relations between child and parent nodes.\n",
    "\n",
    "<img src=\"comp-graph-eval.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Back Propagation\n",
    "\n",
    "Firstly, calculate derivations of nodes and extend computation graph with them by adding back (derivative) relations to nodes. Then one can calculate node values by going by direction from the top node of the graph to its leaf nodes.\n",
    "\n",
    "<img src=\"comp-graph-derivs.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "PLease, take a look on variable $b$ node. To compute derivation one needs to get values of two nodes $c$ and $d$ and sum them. For more complex expressions with more variables such sum may be very sofisticated. To not compute values of $c$ and $d$ again and again one need to **cache** them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Computation Model\n",
    "\n",
    "TensorFlow builds computation graph by basing on your Python code, where graph nodes are defined. Leaf nodes are variables and constants, internal nodes are operations. An each node in the computation graph has its own unique identifier and its value is computed only once and cached after the computation.\n",
    "\n",
    "TensorFlow extend an each node in the computation graph by adding back derivative relation to child nodes. An each node in the graph has relations to child nodes to compute in forward propagation and relations to its parent nodes to compute back propagation.\n",
    "\n",
    "## Using of TensorFlow\n",
    "\n",
    "#### The first phase:\n",
    "Define computation graph in your code by using operations and variables from tf namespace.\n",
    "\n",
    "#### The second phase:\n",
    "Create ccalculation session to TF server and run computations on the graph by passing values of leaf nodes and names of top nodes of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(2.)\n",
    "b = tf.constant(3.)\n",
    "r = tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "e =  6.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.Variable(2., name='a', dtype=tf.float32)\n",
    "b = tf.Variable(1., name='b', dtype=tf.float32)\n",
    "c = tf.add(a, b)\n",
    "d = tf.add(b, tf.constant(1.))\n",
    "e = tf.multiply(c, d)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print('e = ', sess.run(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network\n",
    "\n",
    "\n",
    "## Formula for one neuron\n",
    "\n",
    "<img src=\"artificial_neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "For each neuron in the layer we have:\n",
    "\n",
    "Input: $\\boldsymbol{x} = \\{ x_1, x_2, \\ldots, x_n \\}$\n",
    "\n",
    "Weights: $\\boldsymbol{w} = \\{ w_1, w_2, \\ldots, w_n \\}$\n",
    "\n",
    "Bias: $b$\n",
    "\n",
    "Sum: $s = \\sum_{i=1}^n w_ix_i + b = \\boldsymbol{w} \\boldsymbol{x} + b$\n",
    "\n",
    "Activation function: $y = f(s)$\n",
    "\n",
    "## Network Layer\n",
    "\n",
    "<img src=\"ffn.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "The NN layer can be desribed by:\n",
    "\n",
    "Matrix $W$ with rows as vectors of $\\boldsymbol{w}$ weights for each neuron in the layer\n",
    "\n",
    "Vector $\\boldsymbol{b}$ of bias for each neuron in the layer\n",
    "\n",
    "Sum is implemented as multiplication of matrix $W^T$ to input vector $\\boldsymbol{x}$\n",
    "\n",
    "Vector $\\boldsymbol{b}$ represents activation functions for all neurons in the layer\n",
    "\n",
    "\n",
    "## TensorFlow representation for n-dim values\n",
    "\n",
    "One may notice that in NN we have different dimension values:\n",
    "* Vectors of input, bias and activation functions.\n",
    "* Matrix of weights.\n",
    "* For more complex configurations of NN one has to operate to 3 or more dimensional values.\n",
    "\n",
    "So, TF introduced notion of **tensor** to operate values of different dimention. Tensors like numpy arrays, but they are used inside TF server during execution circle in forward and back propagation calculations. It's possible to pass numpy array as the input values of computation graph before running calculation of the graph inside a session.\n",
    "\n",
    "Tensors have shape, which defines dimension of the tensor. For example, any number has shape=() - 0-dimesional. Vectors have shape=(N), where N is length of the vector. Matrices have shape=(N,M), where N and M are matrix dimensions.\n",
    "\n",
    "One needs a way to pass values as numpy arrays (or something else) to the graph to run computations on the same graph but different values. TensoFlow provides following ways to do it:\n",
    "1. Use predefined values in constants and variable as it was demostrated before.\n",
    "2. Use placeholders.\n",
    "3. Use special data structures to fast value feeding (tf.data.Dataset).\n",
    "\n",
    "### Using Placeholders\n",
    "\n",
    "One should declared placeholder exactly like a variable, by defining shape of the data to keep in the placeholder. Then one can use this placeholder anywhere in the operations of program code. The placeholder node in the computation graph doesn't contain any value yet. One shoul pass (**feed**) this value to placeholder when the computation session is running. TF Session run method has additional parameter 'feed_dict', which is Python dict where one ca pass values by assigning them to placeholders.\n",
    "\n",
    "Let's take a look on the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e =  6.0\n",
      "e =  77.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.placeholder(dtype=tf.float32, shape=())\n",
    "b = tf.placeholder(dtype=tf.float32, shape=())\n",
    "c = tf.add(a, b)\n",
    "d = tf.add(b, tf.constant(1.))\n",
    "e = tf.multiply(c, d)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print('e = ', sess.run(e, feed_dict={a: 2., b: 1.}))\n",
    "    print('e = ', sess.run(e, feed_dict={a: 5., b: 6.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Prediction\n",
    "\n",
    "## MNIST Database\n",
    "\n",
    "<img src=\"mnist-examples.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Train data collection contains 60000 examples, test collection - 10000. Each picture is $28\\times 28$ gray scaled image. \n",
    "\n",
    "\n",
    "## Feed Forward Network for MNIST Prediction\n",
    "\n",
    "<img src=\"ffn-mnist.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "* $X$ - flattened vector of $28\\times 28 = 784$ length.\n",
    "* $Y$ - labels as hot-vector. For instance, 3 = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
    "* $W_1$, $b_1$ - weights and bias for the first layer.\n",
    "* $W_1$, $b_1$ - weights and bias for the second layer.\n",
    "* $H = ReLU(X\\times W_1 + b_1)$\n",
    "* $O = Softmax(H\\times W_2 + b_2)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/vlapshin/.local/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Step 0 Current Training Accuracy: 0.110\n",
      "Step 100 Current Training Accuracy: 0.880\n",
      "Step 200 Current Training Accuracy: 0.920\n",
      "Step 300 Current Training Accuracy: 0.930\n",
      "Step 400 Current Training Accuracy: 0.910\n",
      "Step 500 Current Training Accuracy: 0.930\n",
      "Step 600 Current Training Accuracy: 0.970\n",
      "Step 700 Current Training Accuracy: 0.910\n",
      "Step 800 Current Training Accuracy: 0.920\n",
      "Step 900 Current Training Accuracy: 0.910\n",
      "Test Accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE = 784, 100, 10 \n",
    "\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "# Input placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, INPUT_SIZE))  \n",
    "Y = tf.placeholder(tf.float32, shape=(None, OUTPUT_SIZE))\n",
    "\n",
    "# Hidden layer weights and bias\n",
    "W_1 = tf.get_variable(\"Hidden_W\", shape=[INPUT_SIZE, HIDDEN_SIZE], initializer=initializer)\n",
    "b_1 = tf.get_variable(\"Hidden_b\", shape=[HIDDEN_SIZE], initializer=initializer)\n",
    "\n",
    "# Hidden layes sum and activation\n",
    "hidden = tf.nn.relu(tf.matmul(X, W_1) + b_1)\n",
    "\n",
    "# Output layer weights and bias\n",
    "W_2 = tf.get_variable(\"Output_W\", shape=[100, 10], initializer=initializer)\n",
    "b_2 = tf.get_variable(\"Output_b\", shape=[10], initializer=initializer)\n",
    "\n",
    "# Output layer sum\n",
    "output = tf.matmul(hidden, W_2) + b_2\n",
    "\n",
    "# Loss as crossentropy with softmax\n",
    "loss = tf.losses.softmax_cross_entropy(Y, output)\n",
    "\n",
    "# Accuracy for prediction\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Optimizer is Adam with default learing rate\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Run session\n",
    "BATCH_SIZE, NUM_TRAINING_STEPS = 100, 1000\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables in the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(NUM_TRAINING_STEPS):\n",
    "        batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "        curr_acc, _ = sess.run([accuracy, train_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "        if i % 100 == 0:\n",
    "            print('Step {} Current Training Accuracy: {:.3f}'.format(i, curr_acc))\n",
    "    \n",
    "    # Evaluate on Test Data\n",
    "    print('Test Accuracy: {:.3f}'.format(sess.run(accuracy, feed_dict={X: mnist.test.images, \n",
    "                                                                Y: mnist.test.labels})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
